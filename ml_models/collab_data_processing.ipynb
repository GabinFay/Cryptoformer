{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNr14NT9Hmsv1dhBEUAwTQ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabinFay/Bot/blob/gabin/collab_data_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMg-7J_m6kVg",
        "outputId": "d2a85afe-0fe9-42fe-c85d-6b40ccdba701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/18)\u001b[K\rremote: Counting objects:  11% (2/18)\u001b[K\rremote: Counting objects:  16% (3/18)\u001b[K\rremote: Counting objects:  22% (4/18)\u001b[K\rremote: Counting objects:  27% (5/18)\u001b[K\rremote: Counting objects:  33% (6/18)\u001b[K\rremote: Counting objects:  38% (7/18)\u001b[K\rremote: Counting objects:  44% (8/18)\u001b[K\rremote: Counting objects:  50% (9/18)\u001b[K\rremote: Counting objects:  55% (10/18)\u001b[K\rremote: Counting objects:  61% (11/18)\u001b[K\rremote: Counting objects:  66% (12/18)\u001b[K\rremote: Counting objects:  72% (13/18)\u001b[K\rremote: Counting objects:  77% (14/18)\u001b[K\rremote: Counting objects:  83% (15/18)\u001b[K\rremote: Counting objects:  88% (16/18)\u001b[K\rremote: Counting objects:  94% (17/18)\u001b[K\rremote: Counting objects: 100% (18/18)\u001b[K\rremote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects:   9% (1/11)\u001b[K\rremote: Compressing objects:  18% (2/11)\u001b[K\rremote: Compressing objects:  27% (3/11)\u001b[K\rremote: Compressing objects:  36% (4/11)\u001b[K\rremote: Compressing objects:  45% (5/11)\u001b[K\rremote: Compressing objects:  54% (6/11)\u001b[K\rremote: Compressing objects:  63% (7/11)\u001b[K\rremote: Compressing objects:  72% (8/11)\u001b[K\rremote: Compressing objects:  81% (9/11)\u001b[K\rremote: Compressing objects:  90% (10/11)\u001b[K\rremote: Compressing objects: 100% (11/11)\u001b[K\rremote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 16 (delta 5), reused 16 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects:   6% (1/16)\rUnpacking objects:  12% (2/16)\rUnpacking objects:  18% (3/16)\rUnpacking objects:  25% (4/16)\rUnpacking objects:  31% (5/16)\rUnpacking objects:  37% (6/16)\rUnpacking objects:  43% (7/16)\rUnpacking objects:  50% (8/16)\rUnpacking objects:  56% (9/16)\rUnpacking objects:  62% (10/16)\rUnpacking objects:  68% (11/16)\rUnpacking objects:  75% (12/16)\rUnpacking objects:  81% (13/16)\rUnpacking objects:  87% (14/16)\rUnpacking objects:  93% (15/16)\rUnpacking objects: 100% (16/16)\rUnpacking objects: 100% (16/16), 11.83 KiB | 931.00 KiB/s, done.\n",
            "From https://github.com/GabinFay/Bot\n",
            "   620b3a5..63021ce  gabin      -> origin/gabin\n",
            "Updating 620b3a5..63021ce\n",
            "Fast-forward\n",
            " .gitignore                                     |   1 \u001b[32m+\u001b[m\n",
            " 2D_to_3Dwindows.py                             |  61 \u001b[32m++++++++++++++\u001b[m\n",
            " add_top_gainers_to_db.py                       |  12 \u001b[32m+++\u001b[m\n",
            " anomaly_isolation_forests                      |   0\n",
            " anomaly_pytorch_bceweighting                   | 100 \u001b[32m+++++++++++++++++++++++\u001b[m\n",
            " anomaly_svm_dtw.py                             |  47 \u001b[32m+++++++++++\u001b[m\n",
            " anomaly_svm_encoder_rbf.py                     |  69 \u001b[32m++++++++++++++++\u001b[m\n",
            " anomaly_transformer.py                         |  90 \u001b[32m+++++++++++++++++++++\u001b[m\n",
            " data_processing_pipeline_gainer_classifier.py  | 198 \u001b[32m+++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " gainer_classifier.py                           |  43 \u001b[32m+++++++\u001b[m\u001b[31m---\u001b[m\n",
            " knn_dwt.py                                     |  36 \u001b[32m+++++++++\u001b[m\n",
            " one_class_svm_dtw_probabilistic.py             |  55 \u001b[32m+++++++++++++\u001b[m\n",
            " probabilistic_isolation_forests_matrixtovec.py |  87 \u001b[32m++++++++++++++++++++\u001b[m\n",
            " read_top_gainers.py                            |  24 \u001b[32m++++++\u001b[m\n",
            " top_gainers_special_db.py                      |   8 \u001b[31m--\u001b[m\n",
            " utils.py                                       |  50 \u001b[32m++++++++++++\u001b[m\n",
            " 16 files changed, 860 insertions(+), 21 deletions(-)\n",
            " create mode 100644 2D_to_3Dwindows.py\n",
            " create mode 100644 add_top_gainers_to_db.py\n",
            " create mode 100644 anomaly_isolation_forests\n",
            " create mode 100644 anomaly_pytorch_bceweighting\n",
            " create mode 100644 anomaly_svm_dtw.py\n",
            " create mode 100644 anomaly_svm_encoder_rbf.py\n",
            " create mode 100644 anomaly_transformer.py\n",
            " create mode 100644 data_processing_pipeline_gainer_classifier.py\n",
            " create mode 100644 knn_dwt.py\n",
            " create mode 100644 one_class_svm_dtw_probabilistic.py\n",
            " create mode 100644 probabilistic_isolation_forests_matrixtovec.py\n",
            " create mode 100644 read_top_gainers.py\n",
            " delete mode 100644 top_gainers_special_db.py\n",
            " create mode 100644 utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"gabin.fay@gmail.com\"\n",
        "!git config --global user.name \"GabinFay\""
      ],
      "metadata": {
        "id": "xCo4VuPMXE9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bsxPlraBZa5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxMtnmYv74oz",
        "outputId": "918f3fc0-f25f-45a0-8cac-ede29c7c29f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mBot\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbu74vuL5izU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from _database import Database\n",
        "\n",
        "db = Database(ssh_pem_path='BotAmazon.pem', remote=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the SQL query with the columns you want\n",
        "columns = ['fetched_timestamp', 'market_cap', 'price_change_percentage_24h_in_currency',\n",
        "           'id', 'symbol', 'name', 'market_cap_rank', 'fully_diluted_valuation', 'total_volume',\n",
        "           'high_24h', 'low_24h', 'price_change_percentage_24h', 'market_cap_change_24h',\n",
        "           'market_cap_change_percentage_24h', 'ath_change_percentage', 'ath_date',\n",
        "           'atl_change_percentage', 'atl_date', 'price_change_percentage_1h_in_currency',\n",
        "           'price_change_percentage_7d_in_currency', 'rank_change_1h', 'rank_percent_change_1h',\n",
        "           'rank_change_1d', 'rank_percent_change_1d', 'rank_change_7d', 'rank_percent_change_7d',\n",
        "           'rank_change_start', 'rank_percent_change_start']\n",
        "\n",
        "# Assuming you have a SQLAlchemy engine created as 'engine'\n",
        "query = \"\"\"\n",
        "SELECT * FROM data5000\n",
        "WHERE fetched_timestamp >= '2024-05-03'\n",
        "AND fetched_timestamp < '2024-05-17'\n",
        ";\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_sql_query(query, db.engine)\n",
        "df = df[columns]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "pwxEZIXf5qns",
        "outputId": "05a9d80f-269b-41c8-ffce-f291cc8b4bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c6d94285795f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill NaNs in one column with values from the other\n",
        "df['market_cap'] = df['market_cap'].combine_first(df['fully_diluted_valuation'])\n",
        "df['fully_diluted_valuation'] = df['fully_diluted_valuation'].combine_first(df['market_cap'])\n",
        "\n",
        "# Drop rows where both columns are still NaN\n",
        "df.dropna(subset=['market_cap', 'fully_diluted_valuation'], how='all', inplace=True)\n",
        "\n",
        "#%%\n",
        "df.drop_duplicates(subset=['id', 'fetched_timestamp'], keep='first', inplace=True)"
      ],
      "metadata": {
        "id": "EMKFMDOP5vuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['fetched_timestamp'] = pd.to_datetime(df['fetched_timestamp'])\n",
        "df['fetched_timestamp'] = df['fetched_timestamp'].dt.round('H') #deals with inconsistent fetched_times !\n",
        "\n",
        "df.set_index('fetched_timestamp', inplace=True)\n",
        "df['MA_6h'] = df.groupby('id')['price_change_percentage_1h_in_currency'].rolling('6H').mean().reset_index(0, drop=True)"
      ],
      "metadata": {
        "id": "8YeejjlM53SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.reset_index(inplace=True)\n",
        "df['date_of_first_appearance'] = df.groupby('id')['fetched_timestamp'].transform('min')\n",
        "\n",
        "def binary_classify_change(x):\n",
        "    if x >= 100:\n",
        "        return 'big gainer'\n",
        "    else:\n",
        "        return 'non big gainer'\n",
        "\n",
        "df['class'] = df['price_change_percentage_24h_in_currency'].shift(-24).apply(binary_classify_change)  # Assuming data is hourly"
      ],
      "metadata": {
        "id": "Y_qw-Ttg56k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['market_cap'] < 100000000] #smaller than 100m mcap"
      ],
      "metadata": {
        "id": "V9Vq68uL58hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values_count = df.isnull().sum()\n",
        "print(missing_values_count)"
      ],
      "metadata": {
        "id": "5rcC2gvD5-z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle NaN values\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "IaoVrFK_6AWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['date_of_first_appearance'] = df['date_of_first_appearance'].astype(int)\n",
        "df['ath_date'] = df['ath_date'].astype(int)\n",
        "df['atl_date'] = df['atl_date'].astype(int)\n",
        "\n",
        "df['hour_of_day'] = pd.to_datetime(df['fetched_timestamp']).dt.hour\n",
        "df['day_of_week'] = pd.to_datetime(df['fetched_timestamp']).dt.dayofweek\n",
        "\n",
        "train_df = df[df['fetched_timestamp'] < '2024-05-10']\n",
        "val_df = df[(df['fetched_timestamp'] >= '2024-05-10') & (df['fetched_timestamp'] < '2024-05-14')]\n",
        "test_df = df[df['fetched_timestamp'] >= '2024-05-14']\n"
      ],
      "metadata": {
        "id": "HsgcaJoW6CT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = 48\n",
        "\n",
        "from utils import get_token_tensor_from2D, create_sliding_windows\n",
        "import numpy as np\n",
        "\n",
        "train_tensor = get_token_tensor_from2D(train_df)\n",
        "train_windows = create_sliding_windows(train_tensor, T)\n",
        "test_tensor = get_token_tensor_from2D(test_df)\n",
        "test_windows = create_sliding_windows(test_tensor, T)\n",
        "val_tensor = get_token_tensor_from2D(val_df)\n",
        "val_windows = create_sliding_windows(val_tensor, T)\n",
        "\n",
        "\n",
        "ts_col_num = df.columns.get_loc('fetched_timestamp')\n",
        "id_col_num = df.columns.get_loc('id')\n",
        "class_col_num = df.columns.get_loc('class')\n",
        "symbol_col_num = df.columns.get_loc('symbol')\n",
        "name_col_num = df.columns.get_loc('name')\n",
        "\n",
        "target = train_tensor[:, -1, class_col_num]\n",
        "indices = ~np.isnan(target)\n",
        "filtered_train_tensor = train_tensor[indices, :, :]\n",
        "\n",
        "target = test_tensor[:, -1, class_col_num]\n",
        "indices = ~np.isnan(target)\n",
        "filtered_test_tensor = test_tensor[indices, :, :]\n",
        "\n",
        "target = val_tensor[:, -1, class_col_num]\n",
        "indices = ~np.isnan(target)\n",
        "filtered_val_tensor = val_tensor[indices, :, :]\n",
        "\n",
        "train_index = filtered_train_tensor[:, :, [id_col_num, ts_col_num]]\n",
        "test_index = filtered_test_tensor[:, :, [id_col_num, ts_col_num]]\n",
        "val_index = filtered_val_tensor[:, :, [id_col_num, ts_col_num]]\n",
        "\n",
        "train_labels = filtered_train_tensor[:, :, class_col_num]\n",
        "test_labels = filtered_test_tensor[:, :, class_col_num]\n",
        "val_labels = filtered_val_tensor[:, :, class_col_num]\n",
        "\n",
        "train_tensor_final = np.delete(filtered_train_tensor, [ts_col_num, id_col_num, class_col_num, symbol_col_num, name_col_num], axis=2)\n",
        "test_tensor_final = np.delete(filtered_test_tensor, [ts_col_num, id_col_num, class_col_num, symbol_col_num, name_col_num], axis=2)\n",
        "val_tensor_final = np.delete(filtered_val_tensor, [ts_col_num, id_col_num, class_col_num, symbol_col_num, name_col_num], axis=2)\n"
      ],
      "metadata": {
        "id": "G98AVQqb6C_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "n, window_length, p = train_tensor_final.shape\n",
        "\n",
        "scaler = StandardScaler()\n",
        "tensor_scaled = scaler.fit_transform(train_tensor_final.reshape(n * window_length, p)).reshape(n, window_length, p)\n",
        "\n",
        "val_tensor_scaled = scaler.transform(val_tensor.reshape(-1, p)).reshape(val_tensor.shape)\n",
        "test_tensor_scaled = scaler.transform(test_tensor.reshape(-1, p)).reshape(test_tensor.shape)"
      ],
      "metadata": {
        "id": "qhqiGZun6FiT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}